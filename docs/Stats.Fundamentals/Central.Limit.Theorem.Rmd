---
title: "Four Fundamental Concepts"
author: "Leandro Ledesma"
date: "2024-03-01"
output: html_document
---

The following document is heavily inspired by Chapter 5: Four Fundamental Concepts from Statistics Unplugged 4th Edition (2013) by Sally Caldwell. The information below essentially makes up the logic behind inferential statistics. Thus if these concepts are not well understood then I would advise you to continue to review it or else understanding why we use statistical methods in behavioral research will be bleak. 

Let's first start by covering some jargon, which need to be ingrained into memory. Each one will be covered in more detail in their respective section.

1) Random Sampling

2) Sampling Error

3) The Sampling Distribution of Sample Means

4) The Central Limit Theorem


### Quick background knowledge

In behavioral research, we are very interested in understanding how something works in a specific population. So first, we need to have good idea of what we want our population to be. If not familiar with this term, a population is all existing scores of something in the natural world. It can be any size, but it is typically large and is technically chosen by the researcher and essentially you. To ground this concept further, let's say we are interested in studying undergraduates at the University of Houston- this will be our population. Now, a quick Google search shows that in 2022, the University of Houston had nearly 38,000 undergraduates enrolled ... that is a lot of people. Now if we truly wanted to know something about this population, let's say how well they would perform in an exam, we could go to each student one by one and ask them to participate in out study and take this exam. Assuming somehow you got everyone to say yes, imagine how long that would take? to collect all of this data? It is impractical! By the time you are done, they would all have grand children and you might be close to death.

A better approach?

Through the work of brilliant statisticians, all of this trouble can be drastically reduced! This is because we can obtain information about the characteristics of our population (exam performance from all 38,000 undergraduates at the University of Houston) from a sample. A sample is a smaller group of scores that represents a population. With this in mind, we can now go to the next section. 

### 1. Random Sampling

Above we mentioned a sample, which is a small group of scores from a population. In continuation of our example, this would mean a small number of undergraduate students instead of all 38,000. However, there is a catch. Not any sample of undergraduate students would do. What we need is a sample that was obtained through a specific method known as **random sampling**. 

We will not be discussing the different techniques that give you a random sample, but instead describe the characteristics of a random sample. What makes a random sample special is that everyone who enrolls had an equal chance of being selected. Additionally, the way of their enrollment was also randomized- meaning there were no added rules from the researcher for enrolling subjects, as in enrolling someone who is 20, then 21, then 22, then 20 and repeat to have an equal sample size for all ages. Instead, 30 people could have been enrolled and all be 20 years-old followed by 5 more enrollments who just happened to be a mixture of 21 and 22 years-old. Thus, this might be annoying for a researcher that wanted to do age group comparisons on the test (because they will have a lot of scores for 20 year-olds and not enough scores for 21 and 22 year-olds). Lastly, everyone enrolled has to be selected in a way where ALL combinations are possible. Now, this last rule is interesting because this potentially can mean that our sample could potentially be 100% females, even though in the population it is more evenly split (~ 50%). 

### 2. Sampling Error

Okay then, so we discuss and understand the importance of a sample in terms of giving us valuable information about a population and about how we need to make sure our sample is random. Now, we need to combine these two ideas together to understand sampling error. If we were to propose my study multiple times, to use a sample to understand how all 38,000 undergraduates would perform an exam, and compare my results across these studies, I would see some slight differences in my results. This difference is known as the **sampling error**.

Here it is how it works. Let's start by first allocating a true mean (mu) to my population, and that the mean represents the average performance on my exam which is 70%. If I were to collect 100 samples from this population, each sample containing 40 students, and had them all take the test and then obtain the average test performance for each sample, they would NOT all be 70%. In matter of fact, most if potentially not all would be 70%, but it would be close! This difference between test performance for each sample (sample mean) and the true population performance (population mean) is **sampling error**. This error just comes from the fact that all different types of scores in our sample had an equal chance of being enrolled, and that in some cases that combination of students that make up the sample perform less or more than what would be expected from the population average. This is to distinguish from another type of error, which is **bias** and that is a result from tester error or a poorly developed experiment. Sampling error is just the phenomenon of differences from estimated values of samples and the true population characteristics just because of random sampling- it is to be expected and there is no way around this. 

### 3. The Sampling Distribution of Sample Means and Standard Error

Well then, if the results (statistics) we obtain from our sample mean are not equal to that of the population mean, and there is nothing we can do to avoid this, then is there any point in even learning/doing statistics? Well yes, yes there is. Keep in mind, that even though they do not match exactly, most of the time, it will be pretty darn close! Now, you could just take my word on this or we can actually prove it (scroll below to show how this works). Essentially, if we already know the characteristics of a population, which we can by creating our own dummy data, and then take samples from this dummy data and obtain the sample means, and then compare that mean to that of the population (subtracting)- most of the time the mean will be close and only a very few times will it be kinda not close, and almost never (but still possible) is it something wayy way off. Additionally, the certainty that this will occur every single time we attempt this with a new population is highly dependent on the size of all the samples. If it is a decent size (let's say around 40), then this should occur every time. 

Now then, let's say we obtain a ridiculous number of samples, calculate their means, and subtract it from the population mean. What do we do next? Well, we can plot them as a histogram and get a good idea of the spread of these numbers, which in fact are errors! Think about, we are now graphing the difference between each sample mean and the population mean. Essentially what is that? Well it is error, the error from each sample in terms of it attempting to predict the population mean. And this error goes both ways, it could have overestimated (positive error value) or underestimated (negative error value) the population mean. Additionally when plotted, we will get a normal distribution curve, and it we look at the spread of these values and compare it to the spread of scores in the population, the spread of **sampling distribution of sample means** is highly restricted to the center portion of the population histogram. Thus, this shows that most of the time, we can get a fairly accurate (but not perfect) understanding of our population by using a sample.


### 4. The Central Limit Theorem

The Central Limit Theorem is the conclusion from all the jargon that was just discussed: random sampling, sampling error, and the sampling distribution of sample means. It is essentially a short-cut or the main point of all these principles, and that is that if we take an infinite number of samples of size n from a population and calculate the mean of all the sample means, then that grand mean will be **exactly equal** to the population mean (mu). Additionally, the second component is that if we were to calculate the standard deviation of the distribution of sample means, which is called the **standard error**, that this number would be equal to the population standard deviation divided by the square root of n (sample size used for each sample). For me personally, the first concept definitely makes sense to me intuitively after explaining all of these sections. The latter conclusion, however, as of right now seems fuzzy and we should just memorize it.

### Universal block code settings

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(comment = NULL)

```

### Load in the data manipulation packages first

```{r loading in the packages, warning = FALSE}
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(kableExtra)

```



### Proving these concepts

We are now going to prove the following things mentioned by creating dummy data using R code. We will construct a population of scores using the rnorm() function in R and have the size of this population be 38,000 to be consistent with our example. However, this time we will have them take an IQ test instead of a regular examination as mentioned prior. Thus the scores present in this distribution will represent IQ scores for all undergraduates at the University of Houston. The population mean (mu) will be 100 and the standard deviation will be 15. This means that 95% of all scores in the population will be between 70 and 130 (Figure 1).


The next step is to take 500 samples from our population each with an n size of 40. These numbers will be saved in a list as vectors. We will calculate the mean of IQ scores from each sample and graph them, which will show the **sampling distribution of sample means** (Figure 2). Additionally, we will take our sample means and subtract each from the population mean (mu = 100) and store these errors in a vector that will be used later for graphing (Figure 3).

Lastly, we will calculate the standard deviation of our distribution of sample means (standard error) and then compare it to the standard deviation of the population mean divided by the square root of n. (Hopefully doing this will give me clarity on why this is important).

#### Create a population
```{r creating a population distribution}
# set a seed
set.seed(123)

# Creating a population 
population.IQ.scores <- rnorm(n = 38000, mean = 100, sd = 15)

# Save it as a dataframe for easier graphing later
population.IQ.scores <- data.frame(x = population.IQ.scores)

```

#### Extract 500 samples (n=40)
```{r taking 500 samples}
# Create a for loop and list
samples.500 <- list()

for(ii in 1:500) {
  
# Extract a sample of 40 subjects from the population with replacement
samples.500[[ii]] <-  sample(x = population.IQ.scores$x,
         size = 40,
         replace = TRUE)
  
}

```


#### Take the mean for each sample

```{r take the sample}
# Use sapply to take the mean of each vector in the list
example.IQ.scores <- samples.500 %>%
  sapply(., function(x) mean(x))

# Save as a dataframe for easier graphing later
example.IQ.scores <- data.frame(x = example.IQ.scores)
```

#### Calculate error (population mean - sample means)

```{r calculate error}
# Subtract the population mean by the mean of each sample (saved as a data frame to graph in the next step) 
error <- data.frame(x = mean(population.IQ.scores$x) - example.IQ.scores)
```

#### Graph the population distribution, the sampling distribution of means, and error

```{r graph time, warning = FALSE}
# Graph the population distribution of scores
population.IQ.scores %>%
  ggplot(aes(x = x)) +
  geom_histogram(fill = "white",
                 color = "black") +
  scale_y_continuous(expand = c(0,0), limits = c(0,5000)) +
  xlim(mean(population.IQ.scores$x) - 4*sd(population.IQ.scores$x), 
       mean(population.IQ.scores$x) + 4*sd(population.IQ.scores$x)) +
  labs(title = "All UH Undegraduate IQ scores (n = 38,000)", 
       x = "Distribution of IQ Scores",
       y = "Frequency",
       caption = "Figure 1: Blue line represents the mean and the red lines represent 2 SD in both directions.") +
  theme_classic() +
  theme(plot.title = element_text(size = 18,
                                  hjust = 0.5),
        axis.title = element_text(size = 12, face = "bold"),
        axis.text = element_text(size = 12),
        plot.caption = element_text(size = 13,
                                    hjust = 0)) +
  geom_vline(xintercept= mean(population.IQ.scores$x),
             size = .5,
             color = "blue") +
    geom_vline(xintercept= c(mean(population.IQ.scores$x) + 2*sd(population.IQ.scores$x),
                             mean(population.IQ.scores$x) - 2*sd(population.IQ.scores$x)),
             size = .5,
             color = "red")
  

# Graph the sampling distribution of means
example.IQ.scores %>%
  ggplot(aes(x= x)) +
  geom_histogram(fill = "white",
                 color = "black",
                 bins = 100) +
  scale_y_continuous(expand = c(0,0), limits = c(0,120)) +
  xlim(mean(population.IQ.scores$x) - 4*sd(population.IQ.scores$x), 
       mean(population.IQ.scores$x) + 4*sd(population.IQ.scores$x)) +
  labs(title = "The Sampling Distribution of mean IQ scores", 
       x = "500 sample means of IQ scores",
       y = "Frequency",
       caption = "Figure 2: Blue line represents the mean; each sample mean was calculated from a sample of 40.") +
  theme_classic() +
  theme(plot.title = element_text(size = 18,
                                  hjust = 0.5),
        axis.title = element_text(size = 12, face = "bold"),
        axis.text = element_text(size = 12),
        plot.caption = element_text(size = 13,
                                    hjust = 0)) +
  geom_vline(xintercept= mean(population.IQ.scores$x),
             size = .5,
             color = "blue") +
      geom_vline(xintercept= c(mean(example.IQ.scores$x) + 2*sd(example.IQ.scores$x),
                             mean(example.IQ.scores$x) - 2*sd(example.IQ.scores$x)),
             size = .5,
             color = "red")

# Calculate the error (redundant)
error %>%
  ggplot(aes(x= x)) +
  geom_histogram(fill = "white",
                 color = "black") +
#scale_y_continuous(expand = c(0,0), limits = c(0,60)) +
  labs(title = "The Distribution of Error in Predicting\nPopulation Mean from Sample Means", 
       x = "500 error values",
       y = "Frequency",
       caption = "Figure 3:The error mean is zero; each error was calculated by taking the population mean and subtracting it by the sample means.") +
  theme_classic() +
  theme(plot.title = element_text(size = 18,
                                  hjust = 0.5),
        axis.title = element_text(size = 12, face = "bold"),
        axis.text = element_text(size = 12),
        plot.caption = element_text(size = 13,
                                    hjust = 0))


```

### Take away from the graphs

Figure 1 represents the distribution of all possible scores that make up our population, which is IQ scores from 38,000 UH undergraduates. We see that the population mean is 100 and that 95% of all scores are between 70 and 115.

Figure 2 is the sampling distribution of sample means (mean of IQ scores). These were calculated from 500 samples with each having an n of 40. The x-axis shows the same range as in Figure 1 to drive home the point that overall, from the 500 sample means we collected, 95% of them are all within 95 and 105, which is pretty good. 

Figure 3 is sort of another way of visualizing Figure 2. Here we show the difference between the population mean (mu) and all the sample means. We can see that the mean for it is 0, indicating that most of the sample means were very close if not the population mean, and that most of the sample means were off by a range of -5 to 5, which again is what we concluded from Figure 2.


### Standard error equal to the population SD divided by sqrt(n)

\begin{align*}

SE =\frac {\sigma} {\sqrt{n}}

\end{align*}

Again, I am not sure what this means, however, let's investigate to see if this statement is true. First, let's obtain the standard error (of means) by calculating the standard deviation of the sampling distribution of sample means.

```{r calculating the SE}
# Calculate standard error from the sampling distribution of sampling means
SE <- sd(example.IQ.scores$x)

```

Next, let's calculate the standard deviation of the population, using the same formula as above.

```{r calculating SD}
# Calculate the standard deviation of the population 
SD <- sd(population.IQ.scores$x)

```

We know that n is equal to 40. Let's see then if this checks out.

```{r are they equivalent, echo = FALSE}
paste("Is the standard error",round(SE,2),"equal or similar to sigma divided by sqrt of n?", round(SD/sqrt(40),2))
paste("Yes they look similar.")
```

After reading the book some more, it seems that the reason why this is mentioned is first, the math does check out and we showed this and secondly, it shows the relationship between the standard deviations of both the population and the sampling distribution of sample means. In the population, the standard deviation is equal to 15. However, in our sampling distribution of sample means, its standard deviation (which is called the **standard error of sample means**) is equal to 2.5. It is substantially smaller, only when we take $\sigma$ and divide it by the square root of the sample sizes do the numbers match. 


## I understand the Central Limit Theorem- now what?

If you truly understand what was said above and feel like you can confidently explain it to someone else- then the next thing to do is build on these ideas with the introduction to confidence intervals. More on this in the Home page.

## Central Theorem Practice Problems

[[Click here for practice problems]](https://leoledesma237.github.io/LeoWebsite/Practice.Problems/Central.Limit.Theorem.html)