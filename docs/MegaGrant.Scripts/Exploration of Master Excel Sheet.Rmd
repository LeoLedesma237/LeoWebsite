---
title: "Master Excel Sheet Descriptive Statistics"
author: "Leandro Ledesma"
date: "2024-01-25"
output: html_document
---

### Universal block code settings

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(comment = NULL)

```

# MegaGrant Master Excel Sheet Data Exploration

Several scripts of code were used to organize data on the MegaGrant server. This code looked for specific names of files to count how many of those files were present and to which ID's they belonged to. The types of data are extensive, including demographic variables such as age and gender, behavioral data such as BRIEF, WHOQL, ARFA, etc, and neuroimaging data like several types of EEG studies and an MRI study. 

### Load in the data manipulation pacakges first

```{r loading in the packages, warning = FALSE}
library(tidyverse)
library(ggplot2)
library(readxl)
library(rio)

```

### Load in the data


```{r load in the data}
#The master excel file is located in the \\timesfile4\MIR_Lab\STUDY 1 pathway
# Set working directory
setwd("Y:/STUDY 1")

# Load in the data
masterData <- read_excel("MegaGrant_TBL_Database (LATEST).xlsx")

# Remove missing number variable
# This variable turned out to be inaccurate!
masterData <- masterData %>%
  select(-(Missing_num))
```

### Exploring the data

There are 780 observations and 20 variables in the master excel sheet. Each observation (row) was created by a script that identified unique ID's from thousands of different files in the directory. There are pro's and con's to this method. The pro is that most if not all ID's would be identified from the directory, thus recovering many ID's that were originally not accounted for. The con is that the code will not be able to differentiate between unique correct ID's and unique mistyped ID's. 


```{r data exploration}
# Data dimensions
dim(masterData)

# Unique ID number
unique(masterData$ID) %>% length()

```

### Differentiating between real and mistyped ID's

The best way to approach this problem is by looking at how much available data is present for each unique ID. We would expect real ID's to have many more data accounted for with that same ID than fake ID's. Therefore, we can use a threshold of missing data count for each ID and use that to help us decide. 

Additionally, as mentioned above in 'Exploring the Data,' we have 20 variables, one of which includes the unique ID number. The highest number of missing data someone could have is technically 19, however almost all fake ID's should have 18, since that would indicates they were created from a typo when naming a file, which explains why they would only have one type of data associated with that ID. 


### Counting the number of missing data by row

The original excel sheet has all missing data marked as a '-' in that cell. Thus, we can count the number of '-' per row which will inform us how much data is missing. A quicker way to do this would be to convert all of the '-' into NA's and then use the sum function for each row, which will automatically count the number of NA's for each row.

```{r counting missing data by row}
# Convert all '-' into NA's
masterData <- masterData %>%
  lapply(., function(x) ifelse(x == "-",NA, x)) %>%
  do.call(cbind,.) %>%
  data.frame()

# Count the number of NA's per row
masterData$missing.num <- rowSums(is.na(masterData))

```

### Plot the percentage of missing data number

```{r out.width="120%"}
# Develop a table showing the frequency of missing data
missing.data.plot <- masterData %>%
  mutate(missing.num = factor(missing.num,
                              levels = c(19:0))) %>%
  group_by(missing.num) %>%
  count() %>%
  ungroup() %>%
  mutate(percentage = round(n/sum(n)*100,1))

missing.data.plot %>%
  ggplot(aes(x = missing.num, y = percentage)) +
  geom_bar(stat = "identity",
           fill = "white",
           color = "black") +
  scale_y_continuous(expand = c(0,0), limits = c(0,70)) +
  geom_text(aes(label = paste(format(round(percentage,1),
                                     nsmall = 1),
                              "%", sep = "")),
                nudge_y = 2.75, size = 4) +
  coord_flip() + 
  theme_classic() +
  labs(title = "Percentage of missing data number per\nunique ID in the Master Excel Sheet",
       x = "Number of Missing Data",
       y = "Proportion") +
  theme(plot.title = element_text(size = 20,
                                    hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text = element_text(size = 14),
        plot.caption = element_text(size = 12,
                                    hjust = 0))

```

We can see a large spike in the percentage of missing data per unique ID's around 18. This makes sense since this would indicate a unique ID that was attributed to only one file, which likely means it is a typo. Thus, we could set our threshold for real unique ID's at 17, meaning that any unique ID with more than 17 files missing will be categorized as a fake id.

Confusingly, you might notice that there is one instance (0.3%) of missing 19 files. I am not sure how this is possible but regardless at least we know that they are 100% a fake ID and should be removed.

### Remove ID's that cross the 17 missing data number threshold

The dimensions for the new dataset after removing ID's with more than 17 missing data are now 701 observations by 21 columns (variables). The variable number increased by one since we introduced a new variable that counts the number of missing data per row. 

```{r removing fake IDs}
# Remove ID's that have more than 17 missing data
masterData.remaining <- masterData %>%
  filter(missing.num < 18)

# Dimensions of masterData after removing ID's
dim(masterData.remaining)
```

### Closer inspection of removed ID's

It is good to manually check which ID's we are removing from our original dataset. On closer inspection this ID's are highly likely to be fake. There is no demographic information present, none were apart of the third Study, and surprisingly, most were from N400 or ASR data, indicating that this might be a result of a tester working in either of these data collection. 

```{r closer inspection on removed IDs}
# The code to view this data frame manually
masterData.removed <- masterData %>%
  filter(missing.num > 17) 

```

### Closer inspection of the kept ID's

From these ID's one of them contains an underscore in the name '8222_' thus this ID will be removed as well. The rest look appropriate. Therefore, our final unique ID number is likely to be 700. 

```{r closer inspection on kept IDs}
# The code to view this data frame manually
masterData.kept <- masterData %>%
  filter(missing.num < 18) %>%
  arrange(missing.num)

masterData.kept <- masterData.kept %>%
  filter(ID != "8222_")

# check dimensions of the dataset
dim(masterData.kept)
```


### Saving files

The cleaned master excel sheet will should be saved to obtain accurate demographic information for analyses of interest. Additionally save and send the removed ID's to our colleagues to verify if these ID's are in fact typos or people that dropped the study. 

```{r saving data}
# Set the working directory
setwd("C:/Users/lledesma.TIMES/Documents/Masters Project")

# Save the data
export(masterData.kept, "MegaGrant_TBL_.xlsx")
export(masterData.removed, "MegaGrant_TBL_removed.xlsx")

```


