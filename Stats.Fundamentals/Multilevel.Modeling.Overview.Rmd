---
title: "Multilevel Modeling Using R"
author: "Leandro Ledesma"
date: "2024-05-16"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(comment = "")

```


```{r loading in the packages, warning = FALSE, echo = FALSE}
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(ggpubr)
```


# Chapter 1: Introduction

### Brief overview of the importance for implementing multilevel modeling

The function of this document is to summarize several chapters from 'Practical Multilevel Modeling Using R' by Francis L. Huang. This textbook is tailor made to learn the basics of multilevel modeling (also known as hierarchical linear modeling, etc.) to run appropriate analyses in data that would be best explained from this type of modeling. Essentially, this model builds on multiple regression by introducing the categorization of variables into different levels. The most basic data structure that would require multilevel modeling is two levels, with level-1 units nested within higher-level groups or clusters. A clear example of this can be shown with predicting school performance. Level-1 units in this case could be related to students overall such as IQ, attendance, motivation, etc. For the level-2 units, it could be classes, since it is expected that teaching performance across classrooms would vary, which could also influence the level-1 outcome. It is important in this context to have classes as a level-2 unit in the model, because students within classrooms are more likely to perform similar to each other than other classrooms. Ignoring this fact by running a multiple regression (ignoring classifying variables into lower or higher-level units) would violate the assumption of independence (since in this cases the performance of one student is likely to predict the performance of another student) and will lead to incorrect parameter estimates. Thus, knowing when to use multilevel modeling will lead to better and more accurate results. 


### Group-level and individual-level correlations are not the same

We know when we have hierarchical data because values of a predictor will be shared many subjects and values of another predictor will for the most part vary person from person. In this case, the former would be the level-2 unit and the latter would be the level-1 unit. One interesting finding from a study that analyzed the correlation between literacy rates and foreign born status is that the correlation between a level-1 predictor/outcome and a level-2 predictor/outcome respectively will not be the same. This is not always the case but it just so happens in this example that the level-1 correlation led to a positive relationship while the level-2 correlation (**ecological correlation**) led to a negative relationship- showing the opposite conclusion. 

### Dangers of running a OLS multiple regression with the inclusion of level-2 group predictors

In your dataset, if you see a predictor value (such as group) that is greatly shared by subjects (aka the subjects are nested within this group category) and this predictor is included into a regular OLS multiple regression, then your results **will** be biased. This comes back to the idea that larger sample sizes will in the end make almost everything significant. The larger then n, the smaller than the standard errors become, which makes it easier to reject the null hypothesis when it was not supposed to be rejected (Type I error). 

### Other reasons for using MLM (research questions)

Having hierarchical data where MLM is appropriate can also allow for investigating several types of research questions. For example, we could engage in any of the following:

1) Do two level-2 predictors have an effect on a level-1 outcome?
2) .. come back to this


# Chapter 2: The Unconditional Means Model

We first need to try to understand a bit about notation since more complicated versions of MLM will only build on this notation. We will be discussing the 'level' notation.

### Review of multiple regression notation

Below are the typical notations of a multiple regression. The first shows the notation of a multiple regression in the population (I think). Y represents the scores of the outcome variable and they are equal to the scores of X multiplied by a fixed constant (slope) plus another fixed constant (intercept) plus any error not accounted for in the model. The second notation shows an estimated multiple regression from sample data. In this case, $\hat{Y}$ represents the estimate scores of Y by the inputs of scores from X. The r, which represents error, is removed from this notation because we are interested in the estimated Y scores- its inclusion will just give us Y/the observed scores for the outcome variable. 

\begin{align*}
Y = \beta_{0} + \beta_{1}X_{1} + r
\end{align*}

\begin{align*}
\hat{Y} = b_{1}X_{1} + a
\end{align*}

### Unconditional Means/Null Model

This is typically the first step when running a multilevel model analysis. Start by creating a model with no predictors whatsoever and then look at the variance of the outcome variable. This null model will have two functions, to 1) partition the variance into different levels and to 2) establish baseline levels of variance. 

**Level 1 equation:** Below is an example of a regression without predictors (with additional information in the subscripts that is important). We have the outcome variable expressed as Y (aka the actual observed values for Y) which is equal to the intercept ($\beta_{0j}$) plus error ($r_{ij}$). In this case the intercept refers to the mean of the outcome variable, which is expressed as $\bar{Y}$. However, the subscripts are also very important because it changes the meaning of this a bit. The introduction of the subscript 'i' is nothing new and represents an individual's score. The 'j' subscript changes the meaning quite a bit because it is a notation for group type. Thus, when we mentioned that the intercept would be the mean of the outcome variable ($\bar{Y}$), one would assume this would be the **grand mean** however that is not the case. Instead, this would correspond to the mean of the outcome variable ($\bar{Y}_{j}$) for group j. Additionally, the error ($r_{ij}$) also includes a 'j' subscript. This indicates that the error (residual- distance the observed values are from the fitted values) is dependent on the group mean of j as well. 


\begin{align*}
Y_{ij} = \beta_{0j} + r_{ij}
\end{align*}


**Level 2 equation:** This includes notation that may be less familiar. It is written in the same format that a regression would be but has less common symbols. Starting from the left side, gamma with the two zeroes ($\gamma_{00}$) represents the **grand mean** of the outcome ($\bar{Y}$). This is pretty straight-forward, it takes the function of what a regular regression intercept would when predictors are absent. The second portion complicates things a bit further. $\mu_{0j}$ I think functions similarly to $r$ because it represents the distance the group of mean j is from the grand mean. We can see then that the addition of these two symbols for respective j groups leads to the outcome means of j. Ex: grand mean of Y + residuals of group mean from the grand mean = group means of Y. 


\begin{align*}
\beta_{0j} = \gamma_{00} + \mu_{0j}
\end{align*}

**Combining equations:** We discuss the logic and specifics of the last two equations so we can merge them into one equation that represents multilevel modeling. The final product is the equation shown below. Starting from left to right, the goal of this model is to predict observed values of the outcome ($Y_{ij}$) for each individual in their respective 'j' group by taking the grand mean of the outcome ($\bar{Y}$) and adding two sets of residuals (no predictors). The first set of residuals (level-2) is the distance of the group mean j from the grand mean and the second set (level-1) is the distance of the observed value from the group mean of j. This can make more sense with the graph below.


\begin{align*}
Y_{ij} = \gamma_{00} + \mu_{0j} + r_{ij}
\end{align*}


**Visualizing the combined equation:** As mentioned, one reason for running a multilevel model is the ability to take the variance of the outcome variable ($Y$) and partitioning it into the individual-level and group-level variance. This can be seen in the plot below. We start with the grand mean ($\gamma_{00}$) represented by the dashed horizontal line going through the plot. The group-level variance ($\mu_{0j}$)is the distance of the group means respectively from the grand mean, squared, summed together (one for each observation) and then divided by the degrees of freedom (number of groups minus one- I think). The individual-level variance ($r_{0j}$) starts at the group mean of j and is the distance the observed Y value is from it, squared, summed together, then divided by the degrees of freedom (n-k?). Again, the main point from this visualization of the unconditional means model is that we can split the variance into group and individual levels. But it is interesting how this follows the same idea for one-way ANOVAs, where the variance is split between and within groups. 

```{r creating a graph showing individual and group variance, warning = FALSE, out.width= "60%", fig.align='center', echo = FALSE}
# Set seed
set.seed(123)

# Create some dummy data containing two groups and three individuals per group
dummy.data <- data.frame(groups = rep(c(1, 2), each = 3),
                         individuals = c(1,2,3,4,5,6),
                         group_means = c(2,2,2,5,5,5))

# Plot the points as a scatterplot
dummy.data %>%
  ggplot(aes(x = groups, y = individuals)) + # Add the individual observed y scores
  geom_jitter(width = .1,
              shape = 3,
              size = 3) +
  geom_point(aes(x = groups, y = group_means), # Add the group means of the observed y scores
             shape = 21, 
             size = 4,
             color = "red") +
  geom_hline(yintercept=mean(dummy.data$individuals), linetype="dashed", size=1) + # Add the grand mean of y
  scale_x_discrete(name ="groups", 
                    limits=c("1","2")) +
  geom_segment(aes(x = .75, y = 2, xend = 1.25, yend = 2),size = 1) + # Add group mean 1
  geom_segment(aes(x = 1.75, y = 5, xend = 2.25, yend = 5), size = 1) + # Add group mean 2 
  theme_light() +
  labs(title = "Visualizing the split in variance from the unconditional means/null model")
```

### Exam dataset (Hierarchical data)

We can now move to creating/fitting an unconditional model on a hierarchical dataset in R. Following the examples of the textbook, we are going to load in the 'Exam' data from the "mlmRev" packaged.  

```{r load the exam data, warning = FALSE}
# Load in the pacakage
library(mlmRev)

# Load in the data
data(Exam)
```

```{r getting to know our data, results='asis', echo = FALSE}
cat("- There are",nrow(Exam), "rows in the Exam dataset \n\n")
cat("- The variable school contains",length(unique(Exam$school)), "unique schools \n\n")

```

Let's view the first 6 rows.

```{r fitting an unconditional model, warning = FALSE}
# View the first 6 rows
head(Exam) %>%
  kbl(caption = "First 6 rows of the Exam hierarchical data") %>%
  kable_paper(full_width = F)

```

To get a better look of what our data consists, we can use the 'glimpse' function.

```{r using the glimpse function}
glimpse(Exam)

```

Our outcome of interest for this dataset is 'normexam'. This is the student's standardized exam score. Standardized in this case means they are z-scores, with the mean equaling close to zero and the standard deviation being equal close to 1. 

```{r exploring the outcome variable, warning = F}
# Load in the psych package
library(psych)

# Use the describe function on the outcome variable
round(data.frame(describe(Exam$normexam)),2) %>%
  kbl(caption = "Outcome variable descriptive statistics") %>%
  kable_paper(full_width = F)
```


### Fitting the unconditional/null model on the Exam data set

This might be kind of odd, but we will now be fitting a model of this data with no predictors. As mentioned, the goal of doing this will be to obtain the variance of group-level and within-level and to use this as a baseline for later comparisons with models that include predictors. Also, it is important to mention that this unconditional/null model is also called the **unconditional random intercept model**. 

The code to run a multilevel model is called the 'lmer' function which comes in the lmerTest package. Even though we say there are no predictors in the model, what that really means mathematically and code wise is that we are only using the intercept as a predictor. Additionally, we need to specify what our random effects are. While I am not 100% sure what this means, it seems that we want this to apply to group variables, such as school. Doing this will allow all scores/individuals within schools to produce regression lines with the same slope but with different y-intercepts. Code wise, this part is programmed with the (1|school) argument, where 1 indicates that we want to the intercepts to vary randomly and school is the name of the group/cluster variable of interest. 

```{r creating an unconditional or null multilevel model, warning = FALSE}
# Load in the lmerTest package
library(lmerTest)

# Run an unconditional/null model/unconditional random intercept model
null.model <- lmer(normexam ~ 1 + (1|school), Exam)
```

Above we saved the null model as an object called 'null.model', now we can use the summary function to obtain information about our model. What we are most interested in is the variance portion, which is mentioned under 'Random effects:' We can see that there is a column named variance, the values below indicate the variance for the group-level ($\tau_{00}$ = .172) and the individual-level ($\sigma^2$ = .848). We can also use the section right underneath to double check the number of observations (4059) and the number of schools (65) that the model was fitted on. These components of variance are going to function as a baseline of variance for future models that include predictors. The idea is that the inclusion of predictors will reduce the variance, since some of it can start to be explained. If including predictors does not decrease the variance much, then we can conclude that they are not very good and neither is the model. 

```{r running summary on the unconditional or null multilevel model}
# Print out the summary of the model
summary(null.model)

```

### Computing the intraclass correlation coefficient (ICC)

The last thing we want to do is calculate the **intraclass correlation coefficient (ICC)** or p ("rho"). This is also known as the variance partition coefficient. This can only be calculated from the null model. It basically tells you the proportion of variance of the outcome that can be explained by grouping-level or level-2 unit variables. The ICC can also be interprested as the similarity between two subjects when they are randomly selected from the same group. Thus, the higher the ICC, then the more group is a stronger predictor of the outcome or the more similar subjects within a group are compared to another group. The formulat to calculate the ICC is very simple and shown below.


\begin{align*}
ICC = \frac{\tau_{00}} {\tau_{00} + \sigma^2} = \frac{.172}{.172 + .848} = .169
\end{align*}

These notation are not new, $\tau_{00}$ represents the variance of the outcome variable partitioned by group and $\sigma^2$ is the partitioned variance represented by individuals. Thus, we take the variance explained by group and divide it by the sum of its variance with the variance of individuals and that produces the ICC. In the Exam dataset, the ICC is .169, which means that is how much variance is explained by grouping. It can also be written as "16.9% of the variance in the outcome can be attributed to school-level factors." This can be calculated easily by hand or you can also use the icc function from the performance packaged to double check.

```{r calculating the ICC using the function}
# Calculate the ICC
performance::icc(null.model)
```
Since the ICC is measuring how much grouping can explain variance in the outcome, then it may reasoned that a very low iCC (.01) would indicate the MLM is not needed. This is not always the case!

### Understanding the ICC further 

The ICC is a value that ranges from 0 to 1. 0 indicates that there is no effect of group (group means are not good for explaining variance), which means that if you were to calculate the groups means and compare them, they would all be pretty similar to each other. Additionally, if you were to compare each individual score within each group to each other, that is where the variation would be. On the otherhand, if the ICC was the value of 1, then that would mean that when comparing group means to each other, they would all be pretty different and the variation of individual scores within each group would be similar or the same. 

```{r showing high and low ICC using graphs, echo = FALSE}
# Create a data frame for high ICC
high.ICC <- data.frame(scores = c(1,1,1,2,2,2,3,3,3),
                       individuals = rep(c("Individual1", "Individual2", "Individual3")),
                       group = rep(c("Group1", "Group2", "Group3"), each = 3))

# Create a data frame for low ICC
low.ICC <- data.frame(scores = c(1,2,3,1,2,3,1,2,3),
                      individuals = rep(c("Individual1", "Individual2", "Individual3")),
                      group = rep(c("Group1", "Group2", "Group3"), each = 3))

# Plot the high ICC data frame
high.ICC.plot <- 
high.ICC %>%
  ggplot(aes(x= group, y = scores, fill = individuals)) +
  geom_bar(stat = "identity",
           position = "dodge",
           color = "black") +
  scale_fill_manual(values=c("grey", "darkgrey", "black")) +
  scale_y_continuous(expand = c(0,0), limits = c(0,4)) + 
  labs(title = "ICC of value 1",
       caption = "The graph above shows an ICC of 1\n
       because there is no variability at the individual\n
       level and all the variability is explained by the\n
       group level.") +
  guides(fill="none") +
  theme_classic() +
  theme(plot.title = element_text(size = 14,
                                  hjust = 0.5),
        axis.title = element_text(size = 12, 
                                  face = "bold"),
        axis.text = element_text(size = 12),
        plot.caption = element_text(size = 9,
                                    hjust = 0))



# Plot the low ICC data frame
low.ICC.plot <- 
low.ICC %>%
  ggplot(aes(x= group, y = scores, fill = individuals)) +
  geom_bar(stat = "identity",
           position = "dodge",
           color = "black") +
  scale_fill_manual(values=c("grey", "darkgrey", "black")) +
  scale_y_continuous(expand = c(0,0), limits = c(0,4)) + 
  labs(title = "ICC of value 0",
       caption = "The graph above shows an ICC of 0\n
       because there is no variability at the group\n
       level and all the variability is explained by\n
       the individual level.") +
  guides(fill="none") +
  theme_classic() +
  theme(plot.title = element_text(size = 14,
                                  hjust = 0.5),
        axis.title = element_text(size = 12, 
                                  face = "bold"),
        axis.text = element_text(size = 12),
        plot.caption = element_text(size = 9,
                                    hjust = 0))

# Show both plots together
ggarrange(high.ICC.plot, low.ICC.plot)
```

# Chapter 3: Adding Predictors to a Random Intercept Model

We will now continue with the same dataset from the previous chapter (Exam) and introduce predictors. Let's start by first visualizing the notation of a MLM when a level-1 predictor is included. For the Exam dataset, let's do this for the variable 'standLRT', which is the standardized London Reading Test. Again, our outcome variable is 'normexam', which is the standardized scores on some exam. Including this predictor in notation would look like the following:

**Level 1:** We can make-out some important information from the symbols present and the subscripts as well. In continuation to what we covered in Chapter 2, the i subscript represents individual scores and j represents the group that the scores are within. Starting from left to right, the Y variable represents the scores of the outcome ('normexam') and it's equal to the intercept ($\beta_{0j}$) which varies in value based on group j + the slope ($\beta_{1j}$) of the first predictor 'standLRT' which also varies by group j + any error $r_{ij}$ that the model does not capture. 

\begin{align*}
Y_{ij} = \beta_{0j} + \beta_{1j}standLRT_{ij} + r_{ij}
\end{align*}

**Level 2:** Unfortunately, the textbook does not explain these formulas clearly, just provides them. We can infer that as before, the intercept used in the equation for level 1 ($\beta_{0j}$) is equal to the grand mean of the outcome variable ($\gamma_{00}$) plus the difference of the observed Y values from j group means respectively ($u_{0j}$).Unsure about what $\gamma_{10}$ means. It may be related to it being a fixed effect. For example, in Chapter 2, while not mentioned but confirmed in [centerstat.org](https://centerstat.org/wp-content/uploads/2021/08/MLM-Chapter-2-Notes-2021.pdf), the grand mean of the outcome variable ($\gamma_{00}$) functioned as the fixed effect while $u_{0j}$ and $r_{ij}$ functioned as the group-level and individual-level random effect. Thus, using gamma with the subscript 10 can indicate to the reader that this is the fixed effect for the first predictor. If this interpretation is correct, then we will predict that the inclusion of a second level-1 predictor will result in this notation $\gamma_{20}$

\begin{align*}
\beta_{0j} = \gamma_{00} + u_{0j}
\end{align*}

\begin{align*}
\beta_{1j} = \gamma_{10}
\end{align*}

**Combining equations:** The combined equation gives us the clearest understading of what is going on. This is not confirmed in the book but an interpretation from what makes sense to me. Again, starting from left to right, Y represents scores of the outcome variable while keeping track of each individual in each j group. The outcome is equal to $\gamma_{00}$, which is the grand mean of the outcome variable and a fixed effect. By this notation it is probably functioning as the y-intercept of the model. Next is $\gamma_{10}$, which is another fixed effect. Notice this is infront of the predictor 'standLRT', so that means $\gamma_{10}$ is functioning as the slope of the model and it is **not** changing. Lastly, what follows in the model are the random effects of the group-level ($u_{0j}$) and the individual-level ($r_{ij}$).


\begin{align*}
Y_{ij} = \gamma_{00} + \gamma_{10}standLRT_{ij} + u_{0j} + r_{ij}
\end{align*}

### Summary statistics of standLRT

Just like our outcome variable, the predictor standLRT is comprised of standardized scores. This means that the original scores were transformed by taking each score, subtracting it by the mean and then dividing it by the standard deviation. Thus, the outcome is a vector of scores where the mean is equal to 0 and the standard deviation is equal to 1. We can also call these **z-scores** and acknowledge that this is a form of grand mean centering. 

```{r show the descriptive statistics of the level 1 predictor variable, echo = FALSE}
# Show descriptive statistics of the predictor variable
round(data.frame(describe(Exam$standLRT)),2) %>%
  kbl(caption = "Level-1 predictor variable descriptive statistics") %>%
  kable_paper(full_width = F)
```

### Introducing the predictor into the model (Conditional, random intercept model)

This next code chunk shows the introduction of a level-1 predictor into the model. Time time we will not specify or write in ~1 as we did in the null model since this is by default added in. This means the code will automatically include the intercept as a predictor. Also, the model intuitively knows whether the predictor being added is a level-1 predictor or a level-2, so that does not need to be specified. This model is now called a **conditional, random intercept model**, in which the intercept is allowed to vary by school (1|school).


```{r create the model with standLRT as a predictor}
# Create a new model with one predictor and save it as an object
mod1 <- lmer(normexam ~ standLRT + (1|school), data = Exam)

```

Below is the summary of the model we just created. We set the argument cor = FALSE to prevent printing the Correlation of Fixed Effects which is not meaningful information. For quality control, let's first compare the Number of observations and Number of groups for each model (mod1 vs null). We see that these are the same so that is a good start. Next, we notice that the variance for group-level (school) and individual-level (Residual) both differ in mod1 compared to null.model. This is interesting because it confirms that the inclusion of a level-1 predictor does not only reduce the variance (error) of the outcome explained by level-1 predictors but also the variance of the outcome that is explained by level-2 predictors. Lastly, mod1 introduces a predictor to the model which is shown under Fixed effects. Here we can see the intercept and standLRT. The estimate (slope of the line or $\gamma_{10}$) is present which is `0.563` and it is statistically significant (p<.05). The degrees of freedom showed are called Satterthwaite degrees of freedom (more information in the textbook). Thus visually we can see not only that introducing standLRT into the model explained some of the variance (had a smaller variance output compare to the null model) but that standLRT was also significant. 

```{r print out the summary of mod1}
# Print out the summary of the model
summary(mod1, cor = FALSE)

# Print out the summary of the null model for comparison
summary(null.model, cor = FALSE)


```

### Calculating pseudo R^2

The pseudo R^2 functions the same as the typical R^2 reported in regressions. It is a proportion that describes how much of the outcome variable is explained by the predictors. To calculate the pseudo R^2, we take the variance at level 1 from the null model ($\sigma^2_{NULL}$) and then subtract it by the level 1 variance of the predictor model ($\sigma^2_{FULL}$) and subtract it from the former ($\sigma^2_{NULL}$).


\begin{align*}
Pseudo R^2 = \frac{\sigma^2_{NULL} - \sigma^2_{FULL}}{\sigma^2_{NULL}}
\end{align*}

We can now use this formula to calculate the pseudo R^2 of mod1.

```{r calculating pseudo R2}
# Obtaining the variance of mod 1
mod1.var <- as.data.frame(VarCorr(mod1))[,c(1,4)]

# Obtaining the variance of the null model
null.model.var <- as.data.frame(VarCorr(null.model))[,c(1,4)]

# join the models variance types
joined.model.var <- mod1.var %>%
  left_join(null.model.var, by = "grp")

# Calculate pseudo R^2
joined.model.var<- joined.model.var %>%
  mutate(vcov.x = round(vcov.x,3),
         vcov.y = round(vcov.y,3),
         PseudoR2 = round((vcov.y - vcov.x)/vcov.y,3))

# Print the data frame
joined.model.var %>%
  kbl(caption = "Pseudo R^2 of mod1") %>%
  kable_paper(full_width = F)
```


```{r printing the pseudo R2 of model 1, results='asis', echo = FALSE}
cat("- The Pseudo R^2 of mod1 is ",joined.model.var$PseudoR2[2],". This means that ",paste(joined.model.var$PseudoR2[2]*100," %",sep="")," of the variance in normexam is explained by the predictor standLRT. However, this estimate is biased for we did not calculate a pure within group effect.", sep="")


```


### ICC of the predictor model

Below is the ICC of the predictor model. We can see that the adjusted ICC is 0.142, which is pretty high. Additionally, the book mentions that our results are somewhat contaminated because the estimate is a mixture of both between and within group effects. We are really interested in the effect of the within group, since we introduced a level one predictor. There is a way, however, to obtain a pure within group effect of standLRT.

```{r ICC of the predictor model}
# Calculate the ICC
performance::icc(mod1)

```



### Creating and adding level-2 predictors





